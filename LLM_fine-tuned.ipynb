{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrusere/youTube-comments-Analyzer/blob/main/LLM_fine-tuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmcx6DV6VkNA"
      },
      "source": [
        "### **LLM-Powered Sentiment Analysis Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOgATOBVkNE"
      },
      "source": [
        "1. Introduction and Overview\n",
        "\n",
        "    **What is Sentiment Analysis?**\n",
        "\n",
        "    >> Sentiment analysis, also known as opinion mining, is a field of natural language processing (NLP) that focuses on determining the emotional tone or attitude expressed within a piece of text. It aims to categorize text as positive, negative, or neutral, and sometimes even delve into more nuanced emotions like joy, anger, or sadness.\n",
        "\n",
        "    > * **Why Does Sentiment Analysis Matter?**\n",
        "\n",
        "    > Sentiment Analysis can play a pivotal role in numerous applications, including (but not limited to):\n",
        "\n",
        "    > * **Brand Monitoring:** Track customer opinions about products and services across social media and review platforms.\n",
        "    > * **Market Research:** Gain insights into consumer sentiment towards brands, products, or trends.\n",
        "    > * **Customer Service:** Analyze customer feedback to identify areas for improvement.\n",
        "    > * **Social Media Analysis:** Monitor public sentiment towards events, news, or policies.\n",
        "    > * **Financial Analysis:** Assess market sentiment to make informed investment decisions.\n",
        "\n",
        "    **Limitations in Creating/developing, Training and Testing Sentiment Analysis models**\n",
        "\n",
        "    > Creating, developing, training, and testing Sentiment Analysis (SA) models involves several limitations and challenges. Here are some of the key ones:\n",
        "\n",
        "    > * Lack of Labeled Training Data\n",
        "    >>  * Limited Availability: Acquiring a large and diverse dataset with accurate sentiment labels can be difficult, especially for niche or specialized domains.\n",
        "    >>  * Cost and Time: Annotating data manually is time-consuming and expensive. Crowd-sourcing can introduce noise and inconsistency.\n",
        "    >>  * Quality of Annotations: The subjectivity of sentiment can lead to inconsistent labels even among human annotators.\n",
        "\n",
        "    > * Complexity of Human Emotions\n",
        "    >>  * Subtlety and Nuance: Human emotions are complex and nuanced. Simple positive, negative, and neutral labels often fail to capture the full spectrum of sentiments.\n",
        "    >>  * Context-Dependence: The sentiment of a statement can depend heavily on the context, which might not be captured in the data.\n",
        "\n",
        "    > * Ambiguity and Sarcasm\n",
        "    >>  * Ambiguity: Words and phrases can have different sentiments depending on the context. For example, \"I saw this movie last night\" could be positive or negative based on the speaker's tone and further context.\n",
        "    >>  * Sarcasm and Irony: Detecting sarcasm and irony is particularly challenging for SA models, as they often rely on cultural and contextual clues beyond the text itself.\n",
        "\n",
        "    > * Language and Cultural Differences\n",
        "    >> * Multilingual Challenges: Developing models that work across multiple languages requires extensive resources. Each language might require a separate model or significant adjustments to handle linguistic nuances.\n",
        "    >> * Cultural Differences: Sentiments expressed in different cultures can vary widely, making it hard to generalize models across different demographics.\n",
        "\n",
        "    > *  Domain-Specific Challenges\n",
        "    >>  * Generalization: Models trained on generic datasets may not perform well in specialized domains such as medical or legal texts. Domain-specific models require specialized training data, which is often scarce.\n",
        "    >>  * Jargon and Slang: Different domains use specific jargon and slang that might not be well-represented in general sentiment datasets.\n",
        "\n",
        "    > * Evolving Language\n",
        "    >>  * Language Change: Language and expressions evolve over time, and models need regular updates to stay relevant. New slang, trends, and shifts in meaning can quickly make a model outdated.\n",
        "\n",
        "    > * Technical Challenges\n",
        "    >>  * Feature Extraction: Identifying the right features that capture sentiment effectively is challenging. Simple keyword-based approaches may miss nuances, while more sophisticated methods like embeddings require substantial computational resources.\n",
        "    >>  * Model Complexity: Building models that are both accurate and efficient can be difficult. More complex models like deep neural networks offer better performance but require more data and computational power.\n",
        "\n",
        "    > * Evaluation and Metrics\n",
        "    >>  * Evaluation Metrics: Standard metrics like accuracy, precision, recall, and F1-score may not fully capture the effectiveness of a sentiment analysis model, especially in imbalanced datasets.\n",
        "    >>  * Real-world Testing: Models may perform well on test datasets but struggle with real-world data due to noise, variations in text, and other unforeseen factors.\n",
        "\n",
        "    > Addressing these challenges often requires a combination of advanced techniques, including the use of transfer learning, semi-supervised learning, and the integration of external knowledge sources to improve the robustness and accuracy of Sentiment Analysis models.\n",
        "\n",
        "    **Enter Large Language Models (LLMs)**\n",
        "\n",
        "    > Large Language Models (LLMs) are sophisticated machine learning models that have been trained on massive amounts of text data. They possess a remarkable ability to understand and generate human-like language.  LLMs can be fine-tuned for specific tasks, such as sentiment analysis, allowing them to leverage their vast knowledge and linguistic capabilities to make accurate predictions about the emotional tone of text.\n",
        "\n",
        "    Pretrained Large Language Models (LLMs) like GPT-3, BERT, and their successors have shown considerable promise in addressing many of the challenges in creating, developing, training, and testing Sentiment Analysis (SA) models. Hereâ€™s how they can help:\n",
        "\n",
        "    > * Lack of Labeled Training Data\n",
        "    >>  * Transfer Learning: LLMs are pretrained on vast amounts of data across diverse domains. Fine-tuning these models on a smaller, domain-specific dataset can significantly enhance performance without requiring extensive labeled data.\n",
        "    >>  * Zero-shot and Few-shot Learning: LLMs can perform tasks with little to no specific task-related training data, allowing for sentiment analysis with minimal labeled examples.\n",
        "    \n",
        "    > * Complexity of Human Emotions\n",
        "    >>  * Rich Representations: LLMs capture nuanced language representations, which helps in understanding the subtleties and complexities of human emotions beyond simple positive, negative, and neutral sentiments.\n",
        "    >>  * Context-Awareness: These models consider the context within and around the text, leading to better handling of context-dependent sentiment.\n",
        "\n",
        "    > * Ambiguity and Sarcasm\n",
        "    >>  * Contextual Understanding: LLMs use context to disambiguate meanings and can better detect sarcasm and irony, thanks to their sophisticated language understanding capabilities.\n",
        "    >>  * Contextual Embeddings: By generating context-specific embeddings, LLMs can differentiate between sentiments in ambiguous phrases more effectively.\n",
        "\n",
        "    > * Language and Cultural Differences\n",
        "    >>  * Multilingual Capabilities: Models like mBERT and XLM-R are pretrained on multiple languages, enabling sentiment analysis across different languages without needing separate models for each.\n",
        "    >>  * Cultural Sensitivity: Pretrained LLMs can be fine-tuned on culturally specific data to better capture the nuances and sentiments of different cultural contexts.\n",
        "\n",
        "    > * Domain-Specific Challenges\n",
        "    >>  * Fine-Tuning: LLMs can be fine-tuned on domain-specific datasets, leveraging their general language understanding to quickly adapt to specialized domains.\n",
        "    >>  * Domain Adaptation: Techniques such as continual learning allow LLMs to incorporate new domain-specific jargon and slang without forgetting previously learned information.\n",
        "\n",
        "    > * Evolving Language\n",
        "    >>  * Adaptability: Pretrained models can be periodically fine-tuned on recent data to stay updated with evolving language and trends.\n",
        "    >>  * Dynamic Updates: Ongoing training or incremental updates ensure that LLMs remain relevant and effective as language evolves.\n",
        "\n",
        "    > *  Technical Challenges\n",
        "    >>  * Feature Extraction: LLMs inherently generate rich features and embeddings, reducing the need for manual feature engineering.\n",
        "    >>  * Efficiency Improvements: Although LLMs can be computationally intensive, optimizations like distillation and pruning can make them more efficient for deployment.\n",
        "\n",
        "    By leveraging these capabilities, pretrained LLMs significantly mitigate many of the traditional challenges in sentiment analysis, leading to more robust, accurate, and contextually aware models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5v0EwnVVkNH"
      },
      "source": [
        "This Jupyter Notebook embarks on an exploratory journey into the realm of sentiment analysis for social media comments. Leveraging the power of Large Language Models (LLMs), we will delve into various techniques to extract sentiment from unlabeled text data. The focus will be on exploring different approaches, assessing their strengths and weaknesses, and ultimately uncovering valuable insights hidden within the vast landscape of social media conversations.\n",
        "\n",
        "Social media platforms are teeming with user-generated content, offering a treasure trove of opinions, emotions, and reactions. Understanding the sentiment behind these comments is crucial for businesses, marketers, researchers, and anyone interested in gauging public opinion. However, the sheer volume and unstructured nature of social media data present a challenge for traditional sentiment analysis methods.\n",
        "\n",
        "This notebook harnesses the capabilities of LLMs, which excel at understanding and generating human-like language, to tackle this challenge head-on. We will investigate various strategies, from zero-shot and few-shot learning to fine-tuning pre-trained models, and evaluate their effectiveness in extracting sentiment from unlabeled social media comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4OHdlaabqTGQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a97-Th1xWJIG"
      },
      "outputs": [],
      "source": [
        "#mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "########################################\n",
        "\n",
        "#changing the working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NLP_Data\")\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM9mIU6kdjuT"
      },
      "outputs": [],
      "source": [
        "# Function to save comments data to a JSON file\n",
        "def save_comments_to_json(comments, filename = 'youtube_comments.json'):\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(comments, json_file, indent=4)\n",
        "\n",
        "def load_comments_from_json(filename = 'youtube_comments.json'):\n",
        "    with open(filename, 'r') as json_file:\n",
        "        comments = json.load(json_file)\n",
        "    return comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcrV9B_FdjDm"
      },
      "outputs": [],
      "source": [
        "# read the sample_comments_and_sentiment.csv file from the current working directory\n",
        "\n",
        "sample_comments_and_sentiment_df = pd.read_csv(\"sample_comments_and_sentiment.csv\")\n",
        "sample_comments_and_sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzkkMeCDU3S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the comment_text, sentiment, and sentiment_brief_explanation columns\n",
        "comment_text_list = sample_comments_and_sentiment_df.head(15)['comment_text'].tolist()\n",
        "sentiment_list = sample_comments_and_sentiment_df.head(15)['sentiment'].tolist()\n",
        "sentiment_brief_explanation_list = sample_comments_and_sentiment_df.head(15)['sentiment_brief_explanation'].tolist()\n",
        "\n",
        "# Print the lists\n",
        "print(\"Comment Text:\")\n",
        "print(comment_text_list)\n",
        "\n",
        "print(\"\\nSentiment:\")\n",
        "print(sentiment_list)\n",
        "\n",
        "print(\"\\nSentiment Brief Explanation:\")\n",
        "print(sentiment_brief_explanation_list)\n"
      ],
      "metadata": {
        "id": "U3PbSK7KT0RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrbysiFMViie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch tensorboard transformers datasets accelerate bitsandbytes trl peft"
      ],
      "metadata": {
        "id": "WARkXJeLAxzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbhxjVjuCfCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjUy6RqPmw4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhcs01xBmw1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "DKg91twXEK-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGdK9QgRELje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wPpf3CAVEp15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkbLB5XsE0RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"pytorch version {torch.__version__}\")"
      ],
      "metadata": {
        "id": "vfrSSMVZE8Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"working on {device}\")"
      ],
      "metadata": {
        "id": "bbGffFhdE9s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbB71iPGFEYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define features and labels\n",
        "X = sample_comments_and_sentiment_df['comment_text']\n",
        "y = sample_comments_and_sentiment_df[['sentiment', 'sentiment_brief_explanation']]\n",
        "\n",
        "# First, split into train and temp sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 30% temp\n",
        "\n",
        "# Then, split the temp set into test and evaluation sets\n",
        "X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 50% of temp\n",
        "\n",
        "# Check the sizes of the resulting sets\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")\n",
        "print(f\"Evaluation set size: {len(X_eval)}\")"
      ],
      "metadata": {
        "id": "_cOcEaF4GpyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xa2mPYenMpGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Training Data (Generate Prompt):\n",
        "# This function should generate a prompt for training the model to predict sentiment and provide an explanation for each comment.\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "    Analyze the sentiment of the comment enclosed in square brackets and provide a detailed sentiment label (\"positive\", \"neutral\", \"negative\", \"mixed\", \"humorous\", etc.) and a brief explanation of the sentiment.\n",
        "\n",
        "    Comment: [{data_point[0]}]\n",
        "    Sentiment: {data_point[\"sentiment\"]}\n",
        "    Sentiment Brief Explanation: {data_point[\"sentiment_brief_explanation\"]}\n",
        "    \"\"\".strip()\n",
        "\n",
        "# For Test Data (Generate Test Prompt):\n",
        "# This function should generate a prompt for testing the model, where the sentiment and explanation are left blank for the model to predict.\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "    Analyze the sentiment of the comment enclosed in square brackets and provide a detailed sentiment label (\"positive\", \"neutral\", \"negative\", \"mixed\", \"humorous\", etc.) and a brief explanation of the sentiment.\n",
        "\n",
        "    Comment: [{data_point[0]}]\n",
        "    Sentiment:\n",
        "    Sentiment Brief Explanation:\n",
        "    \"\"\".strip()\n"
      ],
      "metadata": {
        "id": "78X52-ILfKNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Training Data (`X_train`):\n",
        "    - We generate training prompts using the generate_prompt function, which includes both the sentiment and the brief explanation.\n",
        "* Evaluation Data (`X_eval_sampled`):\n",
        "  - The evaluation prompts are generated similarly using the `generate_prompt` function.\n",
        "* Test Data (`X_test and y_true`):\n",
        "  - For the test data, we use the `generate_test_prompt` function, which leaves the sentiment and explanation blank, allowing the model to predict these during test & evaluation.\n",
        "* The `y_true` variable is retained to compare the model's predictions with the actual sentiment labels during evaluation.\n",
        "* Dataset Conversion:\n",
        "  - Finally, we convert the resulting DataFrames into Hugging Face Dataset objects, which are typically used in fine-tuning models with the transformers library."
      ],
      "metadata": {
        "id": "8bzYb7w9ghZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine X_train with y_train into a single DataFrame\n",
        "train_df = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Apply generate_prompt to the combined DataFrame\n",
        "X_train = train_df.apply(generate_prompt, axis=1)\n",
        "\n",
        "# Generate evaluation prompts\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Extract true sentiment labels from the test set for later evaluation\n",
        "y_true = y_test['sentiment'].reset_index(drop=True)\n",
        "\n",
        "# Generate test prompts (where sentiment and explanation are left blank)\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Convert the prepared dataframes into Hugging Face datasets\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)\n",
        "test_data = Dataset.from_pandas(X_test)"
      ],
      "metadata": {
        "id": "75RMTx-MghzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[0][435]"
      ],
      "metadata": {
        "id": "Oywjke0f_wRo",
        "outputId": "9b6ff5f8-2d5e-46ee-8a2d-b4d20f2c5676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Analyze the sentiment of the comment enclosed in square brackets and provide a detailed sentiment label (\"positive\", \"neutral\", \"negative\", \"mixed\", \"humorous\", etc.) and a brief explanation of the sentiment.\\n\\n    Comment: [Yes .. the most beautiful. Jordan is still also the most Fly to ever play on the court. His look, his combination of athleticism, skill, and style/aesthetics, he is without equal imo.]\\n    Sentiment: Positive\\n    Sentiment Brief Explanation: Acknowledges both Jordan and Lebron\\'s greatness but favors Jordan\\'s style.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[0][0]"
      ],
      "metadata": {
        "id": "K3HjRAO4_wl8",
        "outputId": "3b601e25-b065-4b50-c110-d2a07154f794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Analyze the sentiment of the comment enclosed in square brackets and provide a detailed sentiment label (\"positive\", \"neutral\", \"negative\", \"mixed\", \"humorous\", etc.) and a brief explanation of the sentiment.\\n\\n    Comment: [these thumbnails make it seem like a mass nuclear explosion happen]\\n    Sentiment: Negative\\n    Sentiment Brief Explanation: The comment suggests the thumbnails are alarming or disturbing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWAxOkNB_wFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LyVR90E_wBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a sentiment classification model.\n",
        "\n",
        "    This function calculates the accuracy, classification report, and confusion\n",
        "    matrix for a set of true and predicted sentiment labels. The sentiment labels\n",
        "    are dynamically mapped to numeric values based on the unique labels present\n",
        "    in the input data.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    y_true : list or array-like\n",
        "        The true sentiment labels.\n",
        "    y_pred : list or array-like\n",
        "        The predicted sentiment labels.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    None\n",
        "        The function prints the overall accuracy, accuracy per sentiment label,\n",
        "        a detailed classification report, and the confusion matrix.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get unique sentiment labels from y_true\n",
        "    unique_labels = sorted(set(y_true))\n",
        "\n",
        "    # Dynamically create a mapping from labels to numeric values\n",
        "    mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, 1)\n",
        "\n",
        "    # Apply the mapping to y_true and y_pred\n",
        "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
        "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
        "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report for each label\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
        "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
        "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=unique_labels)\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=[mapping[label] for label in unique_labels])\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "Bpfuj1SN3xzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dH_OHpETMs6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the GPT-3 model name\n",
        "model_name = \"gpt-3\"\n",
        "\n",
        "# Set the compute data type\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "# Configure the model to use 4-bit quantization for efficient training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the GPT-3 model with the specified configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "# Disable caching for training\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Adjust pretraining TP if necessary\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load the tokenizer associated with the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Set the padding token and side\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Apply any necessary chat formatting\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./results\")"
      ],
      "metadata": {
        "id": "9LfGguFKMs3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ausG804mrmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}