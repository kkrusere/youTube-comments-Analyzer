{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrusere/youTube-comments-Analyzer/blob/main/LLM_fine-tuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **LLM-Powered Sentiment Analysis Pipeline** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Introduction and Overview\n",
        "\n",
        "    **What is Sentiment Analysis?**\n",
        "\n",
        "    >> Sentiment analysis, also known as opinion mining, is a field of natural language processing (NLP) that focuses on determining the emotional tone or attitude expressed within a piece of text. It aims to categorize text as positive, negative, or neutral, and sometimes even delve into more nuanced emotions like joy, anger, or sadness.\n",
        "\n",
        "    > * **Why Does Sentiment Analysis Matter?**\n",
        "\n",
        "    > Sentiment Analysis can play a pivotal role in numerous applications, including (but not limited to):\n",
        "\n",
        "    > 1. **Brand Monitoring:** Track customer opinions about products and services across social media and review platforms.\n",
        "    > 2. **Market Research:** Gain insights into consumer sentiment towards brands, products, or trends.\n",
        "    > 3. **Customer Service:** Analyze customer feedback to identify areas for improvement.\n",
        "    > 4. **Social Media Analysis:** Monitor public sentiment towards events, news, or policies.\n",
        "    > 5. **Financial Analysis:** Assess market sentiment to make informed investment decisions.\n",
        "\n",
        "    **Limitation in Creating/developing, Training and Testing Sentiment Analysis models**\n",
        "\n",
        "    >  > Creating, developing, training, and testing Sentiment Analysis (SA) models involves several limitations and challenges. Here are some of the key ones:\n",
        "    > 1. Lack of Labeled Training Data\n",
        "    > > * Limited Availability: Acquiring a large and diverse dataset with accurate sentiment labels can be difficult, especially for niche or specialized domains.\n",
        "    > > * Cost and Time: Annotating data manually is time-consuming and expensive. Crowd-sourcing can introduce noise and inconsistency.\n",
        "    > > * Quality of Annotations: The subjectivity of sentiment can lead to inconsistent labels even among human annotators.\n",
        "    > 2. Complexity of Human Emotions\n",
        "    > > * Subtlety and Nuance: Human emotions are complex and nuanced. Simple positive, negative, and neutral labels often fail to capture the full spectrum of sentiments.\n",
        "    > > * Context-Dependence: The sentiment of a statement can depend heavily on the context, which might not be captured in the data.\n",
        "    > 3. Ambiguity and Sarcasm\n",
        "    > > * Ambiguity: Words and phrases can have different sentiments depending on the context. For example, \"I saw this movie last night\" could be positive or negative based on the speaker's tone and further context.\n",
        "    > > * Sarcasm and Irony: Detecting sarcasm and irony is particularly challenging for SA models, as they often rely on cultural and contextual clues beyond the text itself.\n",
        "    > 4. Language and Cultural Differences\n",
        "    > > * Multilingual Challenges: Developing models that work across multiple languages requires extensive resources. Each language might require a separate model or significant adjustments to handle linguistic nuances.\n",
        "    > > * Cultural Differences: Sentiments expressed in different cultures can vary widely, making it hard to generalize models across different demographics.\n",
        "    > 5. Domain-Specific Challenges\n",
        "    > > * Generalization: Models trained on generic datasets may not perform well in specialized domains such as medical or legal texts. Domain-specific models require specialized training data, which is often scarce.\n",
        "    > > * Jargon and Slang: Different domains use specific jargon and slang that might not be well-represented in general sentiment datasets.\n",
        "    > 6. Evolving Language\n",
        "    > > * Language Change: Language and expressions evolve over time, and models need regular updates to stay relevant. New slang, trends, and shifts in meaning can quickly make a model outdated.\n",
        "    > 7. Technical Challenges\n",
        "    > > * Feature Extraction: Identifying the right features that capture sentiment effectively is challenging. Simple keyword-based approaches may miss nuances, while more sophisticated methods like embeddings require substantial computational resources.\n",
        "    > > * Model Complexity: Building models that are both accurate and efficient can be difficult. More complex models like deep neural networks offer better performance but require more data and computational power.\n",
        "    > 8. Evaluation and Metrics\n",
        "    > > * Evaluation Metrics: Standard metrics like accuracy, precision, recall, and F1-score may not fully capture the effectiveness of a sentiment analysis model, especially in imbalanced datasets.\n",
        "    > > * Real-world Testing: Models may perform well on test datasets but struggle with real-world data due to noise, variations in text, and other unforeseen factors.\n",
        "\n",
        "    >  > Addressing these challenges often requires a combination of advanced techniques, including the use of transfer learning, semi-supervised learning, and the integration of external knowledge sources to improve the robustness and accuracy of Sentiment Analysis models.\n",
        "\n",
        "    **Enter Large Language Models (LLMs)**\n",
        "\n",
        "    >> Large Language Models (LLMs) are sophisticated machine learning models that have been trained on massive amounts of text data. They possess a remarkable ability to understand and generate human-like language.  LLMs can be fine-tuned for specific tasks, such as sentiment analysis, allowing them to leverage their vast knowledge and linguistic capabilities to make accurate predictions about the emotional tone of text.\n",
        "\n",
        "    Pretrained Large Language Models (LLMs) like GPT-3, BERT, and their successors have shown considerable promise in addressing many of the challenges in creating, developing, training, and testing Sentiment Analysis (SA) models. Hereâ€™s how they can help:\n",
        "\n",
        "    > 1. Lack of Labeled Training Data\n",
        "    >> Transfer Learning: LLMs are pretrained on vast amounts of data across diverse domains. Fine-tuning these models on a smaller, domain-specific dataset can significantly enhance performance without requiring extensive labeled data.\n",
        "    >> Zero-shot and Few-shot Learning: LLMs can perform tasks with little to no specific task-related training data, allowing for sentiment analysis with minimal labeled examples.\n",
        "    > 2. Complexity of Human Emotions\n",
        "    >> Rich Representations: LLMs capture nuanced language representations, which helps in understanding the subtleties and complexities of human emotions beyond simple positive, negative, and neutral sentiments.\n",
        "    >> Context-Awareness: These models consider the context within and around the text, leading to better handling of context-dependent sentiment.\n",
        "    > 3. Ambiguity and Sarcasm\n",
        "    >> Contextual Understanding: LLMs use context to disambiguate meanings and can better detect sarcasm and irony, thanks to their sophisticated language understanding capabilities.\n",
        "    >> Contextual Embeddings: By generating context-specific embeddings, LLMs can differentiate between sentiments in ambiguous phrases more effectively.\n",
        "    > 4. Language and Cultural Differences\n",
        "    >> Multilingual Capabilities: Models like mBERT and XLM-R are pretrained on multiple languages, enabling sentiment analysis across different languages without needing separate models for each.\n",
        "    >> Cultural Sensitivity: Pretrained LLMs can be fine-tuned on culturally specific data to better capture the nuances and sentiments of different cultural contexts.\n",
        "    > 5. Domain-Specific Challenges\n",
        "    >> Fine-Tuning: LLMs can be fine-tuned on domain-specific datasets, leveraging their general language understanding to quickly adapt to specialized domains.\n",
        "    >> Domain Adaptation: Techniques such as continual learning allow LLMs to incorporate new domain-specific jargon and slang without forgetting previously learned information.\n",
        "    > 6. Evolving Language\n",
        "    >> Adaptability: Pretrained models can be periodically fine-tuned on recent data to stay updated with evolving language and trends.\n",
        "    >> Dynamic Updates: Ongoing training or incremental updates ensure that LLMs remain relevant and effective as language evolves.\n",
        "\n",
        "    > 7. Technical Challenges\n",
        "    >> Feature Extraction: LLMs inherently generate rich features and embeddings, reducing the need for manual feature engineering.\n",
        "    >> Efficiency Improvements: Although LLMs can be computationally intensive, optimizations like distillation and pruning can make them more efficient for deployment.\n",
        "\n",
        "\n",
        "    By leveraging these capabilities, pretrained LLMs significantly mitigate many of the traditional challenges in sentiment analysis, leading to more robust, accurate, and contextually aware models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Jupyter Notebook embarks on an exploratory journey into the realm of sentiment analysis for social media comments. Leveraging the power of Large Language Models (LLMs), we will delve into various techniques to extract sentiment from unlabeled text data. The focus will be on exploring different approaches, assessing their strengths and weaknesses, and ultimately uncovering valuable insights hidden within the vast landscape of social media conversations.\n",
        "\n",
        "Social media platforms are teeming with user-generated content, offering a treasure trove of opinions, emotions, and reactions. Understanding the sentiment behind these comments is crucial for businesses, marketers, researchers, and anyone interested in gauging public opinion. However, the sheer volume and unstructured nature of social media data present a challenge for traditional sentiment analysis methods.\n",
        "\n",
        "This notebook harnesses the capabilities of LLMs, which excel at understanding and generating human-like language, to tackle this challenge head-on. We will investigate various strategies, from zero-shot and few-shot learning to fine-tuning pre-trained models, and evaluate their effectiveness in extracting sentiment from unlabeled social media comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a97-Th1xWJIG",
        "outputId": "610aa171-c503-4663-fd9d-70b7336c19f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/EV NLP Data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "#mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "########################################\n",
        "\n",
        "#changing the working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NLP Data\")\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i4C4yBdIWJIH"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kaggle\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !mv kaggle.json ~/.kaggle/\n",
        "# !kaggle datasets download -d widhiwinata/twitter-and-reddit-sentiment-analysis\n",
        "# !unzip twitter-and-reddit-sentiment-analysis.zip\n",
        "# !rm twitter-and-reddit-sentiment-analysis.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8bx1miqj7K",
        "outputId": "77527353-e464-4a6c-fb2b-111d59924e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qyWzzKcsktih"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w0fhAH5eIsA"
      },
      "source": [
        "We are going to do a semi supervised lable propergation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHoKkOovgewR",
        "outputId": "39832b33-1c05-47cd-9fa3-97eb656fecb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
