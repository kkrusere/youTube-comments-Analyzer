{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIro9FcmWcFPtMhhrQSArL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a00c0f17ef6d4e4a9242c36ca4342f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58ba4777288c4090b937b93aaa1a2d9a",
              "IPY_MODEL_e9f5b1bf11b346cdb8decc51ad3d273a",
              "IPY_MODEL_e0b6f28cc8c54feea0db56ef0713cbc4"
            ],
            "layout": "IPY_MODEL_579ba9e19c9b43b6b59f2b19b67628ba"
          }
        },
        "58ba4777288c4090b937b93aaa1a2d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd2a0eedea94603ad9d15bc374f1d04",
            "placeholder": "​",
            "style": "IPY_MODEL_57dd0e3d821d41cf9ef14d156a853747",
            "value": "Map: 100%"
          }
        },
        "e9f5b1bf11b346cdb8decc51ad3d273a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25d20d5f514c47b3be0781bcad46b3f9",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fdfe3a7255b4f3e947d652041f5f8b4",
            "value": 100
          }
        },
        "e0b6f28cc8c54feea0db56ef0713cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b99019fec8824d869a26f5aa65c0c373",
            "placeholder": "​",
            "style": "IPY_MODEL_0c4c2a3a4c734c72a8a3764068989985",
            "value": " 100/100 [00:00&lt;00:00, 618.12 examples/s]"
          }
        },
        "579ba9e19c9b43b6b59f2b19b67628ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd2a0eedea94603ad9d15bc374f1d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57dd0e3d821d41cf9ef14d156a853747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25d20d5f514c47b3be0781bcad46b3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fdfe3a7255b4f3e947d652041f5f8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b99019fec8824d869a26f5aa65c0c373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4c2a3a4c734c72a8a3764068989985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrusere/youTube-comments-Analyzer/blob/main/SAnalysis_on_YT_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>  **Sentiment Analysis and Explanation Generation with BART and LoRA**</center>\n",
        "\n",
        " The notebook illustrates the fine-tuning of a BART model (`\"facebook/bart-large-cnn\"`)(with LoRA for efficient adaptation) to generate `sentiment labels` and `explanations` for YouTube video comments, based on the video's title, description, and comment text. It uses the Hugging Face Trainer API for streamlined training, evaluation, and deployment to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "TB8-gWVJt0u6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MY8TIoD1vLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56xfFNgWvMEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **1. Setup and Dependencies Installation**\n",
        "The first section installs necessary libraries:\n",
        "- `bitsandbytes`: for efficient 8-bit optimization of models (especially useful for large models like BART).\n",
        "- `accelerate`: helps optimize training for multiple devices (e.g., GPUs).\n",
        "- `trl, peft`: for task-specific fine-tuning using techniques like LoRA.\n",
        "- `datasets, evaluate, rouge-score`: for data management and evaluation metrics.\n",
        "- `huggingface_hub`: for interacting with Hugging Face's model hub."
      ],
      "metadata": {
        "id": "yde9TDyHvM-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install bitsandbytes\n",
        "pip install accelerate\n",
        "pip install trl peft\n",
        "pip install datasets\n",
        "pip install rouge-score\n",
        "pip install evaluate\n",
        "pip install huggingface_hub\n"
      ],
      "metadata": {
        "id": "slYfYmt_p59Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pr2KYspSvhLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **2. Import Libraries**\n",
        "We will import all the required libraries for data manipulation, model training, and evaluation.\n"
      ],
      "metadata": {
        "id": "qyyU7CW5viAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "from transformers import (\n",
        "                            AutoModelForCausalLM,\n",
        "                            AutoTokenizer,\n",
        "                            BartForConditionalGeneration,\n",
        "                            BartTokenizer,\n",
        "                            BitsAndBytesConfig,\n",
        "                            EarlyStoppingCallback,\n",
        "                            logging,\n",
        "                            pipeline,\n",
        "                            Trainer,\n",
        "                            TrainingArguments,\n",
        ")\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UDGOEFoOp55W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nG3-wREPp518"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuzSshz2wTXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **3. Mount Google Drive and HuugingFace login**\n",
        "\n",
        "- `from google.colab import drive, userdata`: Imports modules specific to Google Colab for interacting with Google Drive and user data storage.\n",
        "- `from huggingface_hub import login`: Imports the login function from the Hugging Face Hub library for authentication.\n",
        "Mount Google Drive:\n",
        "- We then mount the Google Drive to the Colab virtual machine, making its\n",
        "- After we change the working directory to the `\"NLP_Data\"` folder within the Google Drive.\n",
        "- `huggingface_token = userdata.get('Hugging_Face_Hub_API_TOKEN')`: Retrieves the `Hugging Face Hub API token` from Colab's user data storage.\n",
        "- `login(huggingface_token, add_to_git_credential=True):` Logs the notebook into the Hugging Face Hub using the retrieved token and adds it to the Git credentials for future use.\n",
        "\n"
      ],
      "metadata": {
        "id": "G_9teas4wULy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "import os\n",
        "import json\n",
        "#mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "########################################\n",
        "\n",
        "#changing the working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NLP_Data\")\n",
        "\n",
        "!pwd\n",
        "\n",
        "huggingface_token = userdata.get('Hugging_Face_Hub_API_TOKEN')\n",
        "\n",
        "#logging into huggingface\n",
        "login(huggingface_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "DKnCtc2KqiSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaGCQylMp5y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **4. Load and Explore Data**\n",
        "Next, we will load the training data and inspect the first few rows to understand its structure. The data contains the following columns of interest:\n",
        "- `channel_name`: Name of the channel\n",
        "- `video_title`: Title of the YouTube video\n",
        "- `video_description`: Description of the YouTube video\n",
        "- `comment_text`: Comment on the video\n",
        "- `Sentiment`: Sentiment label for the comment (e.g., Positive, Negative)\n",
        "- `Explanation`: Explanation of the sentiment\n"
      ],
      "metadata": {
        "id": "5wmM0e2JyI2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP_Data/train_valid_data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "T47MKtfDqkOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"\"\"\n",
        "        Channel Name: {df['channel_name'][0]}\n",
        "        Video Title: {df['video_title'][0]}\n",
        "        Description: {df['video_description'][0]}\n",
        "        Comment Text: {df['comment_text'][0]}\n",
        "        \\n\n",
        "        Sentiment: {df['Sentiment'][0]}\n",
        "        Explanaition: {df['Explanation'][0]}\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "#returns\n",
        "        # Channel Name: BBC\n",
        "        # Video Title: Can Cuttlefish camouflage in a living room? | Richard Hammond's Miracles of Nature - BBC\n",
        "        # Description: The final episode of Richard Hammond’s Miracles Of Nature. Richard is once again investigating the extraordinary super-powers of the animal kingdom. Cuttlefish survive by being able to blend into their surroundings through camouflage. Richard Hammond puts this to the test and experiments if the fish are able to camouflage in a tank set up like a living room.\n",
        "        # Comment Text: The big white square on his back was impressive af even tho it wasn't fooling our human perception.\n",
        "\n",
        "\n",
        "        # Sentiment: Positive\n",
        "        # Explanaition: The comment expresses admiration for the cuttlefish's camouflage abilities, despite it not being completely convincing to humans.\n"
      ],
      "metadata": {
        "id": "JRDu339yqkBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJ4v20TQq2zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **5. Train-Validation Split**\n",
        "We will split the data into training and validation sets using an 80-20 split.\n"
      ],
      "metadata": {
        "id": "u6-bFKYiyfFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP_Data/test_data.csv')\n",
        "train_valid_data = pd.read_csv('/content/drive/MyDrive/NLP_Data/train_valid_data.csv')\n",
        "\n",
        "# Split the dataset into training and validation sets (80-20 split)\n",
        "train_df, val_df = train_test_split(train_valid_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hjW9CpR8yiyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpRec72Iy7H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **6. Initialize Model and Tokenizer**\n",
        "We will now initialize the BART model (`facebook/bart-large-cnn`) and its corresponding tokenizer for our fine-tuning task.\n"
      ],
      "metadata": {
        "id": "SI4gIh7xyk2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "model_name = \"facebook/bart-large-cnn\"  # BART model name\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Eqy4N743yuo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78TGkeJEy6po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **7. Data Formatting**\n",
        "To prepare the data for training, we will format each input row by combining the `channel_name`, `video_title`, `video_description`, and `comment_text` into a single input text. The output will be the `Sentiment` and `Explanation`.\n"
      ],
      "metadata": {
        "id": "zUaLRk-WywPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "def format_data(df, for_test=False):\n",
        "    return [\n",
        "        {\n",
        "            \"input\": f\"Channel: {row['channel_name']}, Title: {row['video_title']}, Description: {row['video_description']}, Comment Text: {row['comment_text']}\",\n",
        "            \"output\": f\"Sentiment: {row['Sentiment']}, Explanation: {row['Explanation']}\" if not for_test else \"Sentiment: , Explanation: \"\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "# Format the data\n",
        "formatted_train_data = format_data(train_df)\n",
        "formatted_val_data = format_data(val_df)\n",
        "formatted_test_data = format_data(test_df, for_test=True)\n",
        "\n",
        "# Convert to Dataset objects\n",
        "train_dataset = Dataset.from_list(formatted_train_data)\n",
        "val_dataset = Dataset.from_list(formatted_val_data)\n",
        "test_dataset = Dataset.from_list(formatted_test_data)"
      ],
      "metadata": {
        "id": "0HPU23cCy47Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-OTAOw_y511"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **9. Tokenization**\n",
        "We need to tokenize the input and output text so that it can be fed into the BART model. We'll create a helper function to handle this process.\n"
      ],
      "metadata": {
        "id": "8msdYc1UzBCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_data(example):\n",
        "    model_inputs = tokenizer(\n",
        "        example[\"input\"],\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example[\"output\"],\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_data, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_data, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "OFsRduAZq2wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYma0ErVq2tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **10. LoRA (Low-Rank Adaptation) Configuration**\n",
        "LoRA allows us to fine-tune the model efficiently by adapting only a subset of parameters. We will configure LoRA to only fine-tune specific layers of the model.\n"
      ],
      "metadata": {
        "id": "kZgLCQ1SzLTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "lora_model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "5F4rpyttzErC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIQBDcimzEe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **11. Training Configuration**\n",
        "Now, we will set up training parameters, such as the number of epochs, batch size, learning rate, and evaluation strategy. Early stopping will also be used to prevent overfitting.\n"
      ],
      "metadata": {
        "id": "7KM5nRfOzQ0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments with Optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=24,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=1000,                 # Increased warmup steps\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    gradient_accumulation_steps=8,     # Simulate larger batch size\n",
        "    fp16=True,                         # Mixed precision training\n",
        "    learning_rate=1e-5,                # Optimized learning rate\n",
        "    lr_scheduler_type=\"linear\",        # Linear decay\n",
        "    load_best_model_at_end=True,       # Save best model\n",
        "    metric_for_best_model=\"eval_loss\", # Track best model by validation loss\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)"
      ],
      "metadata": {
        "id": "_PVwOR0Qq2qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **12. Trainer Initialization**\n",
        "We will initialize the `Trainer` class, which will handle the training and evaluation processes.\n"
      ],
      "metadata": {
        "id": "S8Z4H92xzbp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer with Early Stopping\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ojypGZgbzeQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoRnM_BhzhKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **14. Evaluation**\n",
        "Once the model is trained, we can evaluate its performance on the validation dataset.\n"
      ],
      "metadata": {
        "id": "p2Xs5MnRzkHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results: {eval_results}\")"
      ],
      "metadata": {
        "id": "Zmr2N0T9q2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eNe0mJv3rWWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **15. Save the Model**\n",
        "After training and evaluation, we will save the fine-tuned model and tokenizer locally for future use.\n"
      ],
      "metadata": {
        "id": "HmYWAdh4zrMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Fine-Tuned Model\n",
        "lora_model.save_pretrained(\"./SA-bart-fine-tuned-lora-model\")\n",
        "tokenizer.save_pretrained(\"./SA-bart-fine-tuned-lora-model\")"
      ],
      "metadata": {
        "id": "AZOh0pUdq2iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **16. Push to Hugging Face Hub**\n",
        "Finally, we will push the model to Hugging Face Hub for sharing or further use in other projects.\n"
      ],
      "metadata": {
        "id": "Cwzhux8BztuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hugging Face Hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "lora_model.push_to_hub(\"kkrusere/SA-bart-fine-tuned-lora-model\")\n",
        "tokenizer.push_to_hub(\"kkrusere/SA-bart-fine-tuned-lora-model\")"
      ],
      "metadata": {
        "id": "_2w71UNLq2e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wi7WX8G5rTGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzB0T1WqrTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **17. Inference on Test Data**\n",
        "To perform inference on the test dataset, we need to format the test data similarly to the training data. We then tokenize the data and use the trained model for predictions.\n"
      ],
      "metadata": {
        "id": "LEYQ84Rlz4m-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2n8eGJGz4Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model = BartForConditionalGeneration.from_pretrained(\"./SA-bart-fine-tuned-lora-model\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"./SA-bart-fine-tuned-lora-model\")\n",
        "\n",
        "# Load the test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP_Data/test_data.csv')\n",
        "\n",
        "# Prepare the test data for inference\n",
        "def format_test_data(df):\n",
        "    return [\n",
        "        {\n",
        "            \"input\": f\"Channel: {row['channel_name']}, Title: {row['video_title']}, Description: {row['video_description']}, Comment Text: {row['comment_text']}\",\n",
        "            \"output\": \"\"  # For test data, the output is not needed\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "# Format and convert test data to Dataset\n",
        "formatted_test_data = format_test_data(test_df)\n",
        "test_dataset = Dataset.from_list(formatted_test_data)\n",
        "\n",
        "# Tokenize the test data\n",
        "def tokenize_data(example):\n",
        "    model_inputs = tokenizer(\n",
        "        example[\"input\"],\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the test dataset\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_data, batched=True)\n",
        "\n",
        "# Inference\n",
        "def infer(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for example in dataset:\n",
        "        inputs = tokenizer(example['input'], return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        outputs = model.generate(**inputs)\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Get predictions\n",
        "predictions = infer(model, tokenizer, tokenized_test_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a00c0f17ef6d4e4a9242c36ca4342f07",
            "58ba4777288c4090b937b93aaa1a2d9a",
            "e9f5b1bf11b346cdb8decc51ad3d273a",
            "e0b6f28cc8c54feea0db56ef0713cbc4",
            "579ba9e19c9b43b6b59f2b19b67628ba",
            "4fd2a0eedea94603ad9d15bc374f1d04",
            "57dd0e3d821d41cf9ef14d156a853747",
            "25d20d5f514c47b3be0781bcad46b3f9",
            "8fdfe3a7255b4f3e947d652041f5f8b4",
            "b99019fec8824d869a26f5aa65c0c373",
            "0c4c2a3a4c734c72a8a3764068989985"
          ]
        },
        "id": "qSZi7GINLjDM",
        "outputId": "b70879c9-0148-4971-b1da-0b5cf06eeb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a00c0f17ef6d4e4a9242c36ca4342f07"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ur9vKDw8Bc8h",
        "outputId": "51e28f23-8118-4140-ad10-6ceca9b97d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sentiment: Negative, Explanation: The comment expresses negative feelings about Tom Hanks, suggesting a negative reaction to the film, suggesting that the actor's dislike of Hanks is the biggest crime of all. The comment has a negative tone, suggesting negative feelings toward Hanks and the film.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5d68zcpbIyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}